# **From Shadows to Details: Leveraging Multi-Scale Wavelet Features for Low-Light Image Enhancement**

xxx, xxx, xxx (names of paper)

Yunnan University(YNU)

## Introduction

This repository is the **official implementation** of the paper, "From Shadows to Details: Leveraging Multi-Scale Wavelet Features for Low-Light Image Enhancement", where more implementation details are presented.

## Overall

![network1](images/network1.svg)

## Dataset

You can refer to the following links to download the datasets:

LOL: Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. "Deep Retinex Decomposition for Low-Light Enhancement", BMVC, 2018. [[Baiduyun (extracted code: wedn)]](链接：https://pan.baidu.com/s/1whZBAKc1NVODqZ2bSEyYXA) [[Google Drive]](https://drive.google.com/file/d/1gh7-7nPonG7f4bBWLkPNAHTQHNV-HNZ4/view?usp=sharing) <br>
LOL-v2 (the extension work): Wenhan Yang, Haofeng Huang, Wenjing Wang, Shiqi Wang, and Jiaying Liu. "Sparse Gradient Regularized Deep Retinex Network for Robust Low-Light Image Enhancement", TIP, 2021. [[Baiduyun (extracted code: wedn)]](链接：https://pan.baidu.com/s/1pn3UffJ_bOlCpNXzehq46g) [[Google Drive]](https://drive.google.com/file/d/1fu1l6irFcSJ5XrUk-JmECQzBf_k2ScYb/view?usp=sharing) <br> <br>

## Results

The evauluation results on LOL are as follows:

![lol_result](images/lol_result.png)

## Get Started

### Requirements

1. Python 3.8
2. Pytorch 2.0.0
3. torchvision 0.17.0
4. cuda 12.1

WEDNet does not need special configurations. Just basic environment.





## Citation

If you find our work useful for your research, please cite our paper

```
@inproceedings{wu2023skf,
  title={Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement},
  author={Yuhui, Wu and Chen, Pan and Guoqing, Wang and Yang, Yang and Jiwei, Wei and Chongyi, Li and Heng Tao Shen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={},
  year={2023}
}
```

## Contact

If you have any question, please feel free to contact us via wuyuhui132@gmail.com.

